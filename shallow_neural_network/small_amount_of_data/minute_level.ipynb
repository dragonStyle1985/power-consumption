{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, batch_size_, win_len_, input_dim_, hidden_dim_, output_dim_):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim_ = hidden_dim_\n",
    "        self.batch_size = batch_size_\n",
    "        self.lstm = nn.LSTM(input_dim_, hidden_dim_, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim_, output_dim_)\n",
    "\n",
    "    def forward(self, x_):\n",
    "        lstm_out, _ = self.lstm(x_)\n",
    "        return self.fc(lstm_out[:, -1, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0   1   2   3   4   5   6   7   8   9    ... 361  362  363  \\\n",
      "0  2011-01-01 00:15:00   0   0   0   0   0   0   0   0   0  ...   0  0.0    0   \n",
      "1  2011-01-01 00:30:00   0   0   0   0   0   0   0   0   0  ...   0  0.0    0   \n",
      "2  2011-01-01 00:45:00   0   0   0   0   0   0   0   0   0  ...   0  0.0    0   \n",
      "3  2011-01-01 01:00:00   0   0   0   0   0   0   0   0   0  ...   0  0.0    0   \n",
      "4  2011-01-01 01:15:00   0   0   0   0   0   0   0   0   0  ...   0  0.0    0   \n",
      "\n",
      "  364 365  366 367 368 369 370  \n",
      "0   0   0    0   0   0   0   0  \n",
      "1   0   0    0   0   0   0   0  \n",
      "2   0   0    0   0   0   0   0  \n",
      "3   0   0    0   0   0   0   0  \n",
      "4   0   0    0   0   0   0   0  \n",
      "\n",
      "[5 rows x 371 columns]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 424. GiB for an array with shape (51296, 3000, 370) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 43\u001B[0m\n\u001B[0;32m     39\u001B[0m windowed_target \u001B[38;5;241m=\u001B[39m target[window_size\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m     41\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m---> 43\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindowed_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindowed_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(torch\u001B[38;5;241m.\u001B[39mTensor(X_train), torch\u001B[38;5;241m.\u001B[39mTensor(y_train))\n\u001B[0;32m     46\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32mD:\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2471\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2467\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m   2469\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[1;32m-> 2471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2472\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_iterable\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2473\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43marrays\u001B[49m\n\u001B[0;32m   2474\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2475\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2473\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   2467\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m   2469\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[0;32m   2471\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2472\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m-> 2473\u001B[0m         (\u001B[43m_safe_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m, _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2474\u001B[0m     )\n\u001B[0;32m   2475\u001B[0m )\n",
      "File \u001B[1;32mD:\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\__init__.py:361\u001B[0m, in \u001B[0;36m_safe_indexing\u001B[1;34m(X, indices, axis)\u001B[0m\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_array_indexing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001B[1;32mD:\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001B[0m, in \u001B[0;36m_array_indexing\u001B[1;34m(array, key, key_dtype, axis)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    184\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m--> 185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marray\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m array[:, key]\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 424. GiB for an array with shape (51296, 3000, 370) and data type float64"
     ]
    }
   ],
   "source": [
    "file_path = \"LD2011_2014.txt\"\n",
    "shift_unit = 24 * 4 * 30\n",
    "total_rows = sum(1 for _ in open(file_path, encoding='utf-8')) - 1  # 减去1是为了排除标题行\n",
    "\n",
    "data = pd.read_csv(file_path, sep=';', header=None, skiprows=1, low_memory=False, nrows=70000)  # 1个月有2880行\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# 假设时间戳列为第一列（索引为0）\n",
    "timestamp_column = 0\n",
    "\n",
    "# 将时间戳列转换为 pandas 的 datetime 类型\n",
    "data[data.columns[timestamp_column]] = pd.to_datetime(data.iloc[:, timestamp_column])\n",
    "\n",
    "# 去除数据中的逗号并转换为浮点数\n",
    "data[data.columns[1:]] = data[data.columns[1:]].replace(',', '', regex=True).astype(float)\n",
    "\n",
    "# 计算相对时间值（以分钟为单位）\n",
    "# reference_time = data.iloc[0, timestamp_column]  # 参考时间点，这里选择第一个时间戳作为参考\n",
    "# data['relative_time'] = (data.iloc[:, timestamp_column] - reference_time).dt.total_seconds() / 60 / 15\n",
    "\n",
    "# 移除原始时间戳列\n",
    "data = data.drop(columns=[timestamp_column])\n",
    "\n",
    "# Extract the target variable (electricity consumption)\n",
    "data_rolled = data.iloc[:, 4:7].rolling(window=shift_unit, min_periods=1).sum().shift(-shift_unit)\n",
    "# To convert values in kWh values must be divided by 4.\n",
    "target = data_rolled.iloc[:-shift_unit].sum(axis=1).values / 4\n",
    "data = data.iloc[:-shift_unit].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "window_size = 3000  # 滑动窗口的大小\n",
    "\n",
    "# 使用 sliding_window_view 函数创建滑动窗口的视图\n",
    "windowed_data = np.lib.stride_tricks.sliding_window_view(data_normalized, (window_size, data.shape[1])).squeeze(1)\n",
    "windowed_target = target[window_size-1:]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(windowed_data, windowed_target, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the hyperparameters\n",
    "input_dim = data.shape[1]  # Number of input features (excluding the timestamp column)\n",
    "hidden_dim = 64  # Number of hidden units\n",
    "output_dim = 1  # Number of output predictions\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the model\n",
    "model = LSTMModel(batch_size, window_size, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(train_dataloader, desc='Training Progress', leave=False)\n",
    "    pbar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    for batch_data in pbar:\n",
    "        # Forward pass\n",
    "        x, y = batch_data\n",
    "        train_outputs = model(x)\n",
    "        loss = criterion(train_outputs, y.unsqueeze(1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "        pbar.update()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "y_true = []  # 存储真实标签值\n",
    "y_pred = []  # 存储预测值\n",
    "\n",
    "# 禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_dataloader:\n",
    "        x, y = batch_data\n",
    "        outputs = model(x)\n",
    "\n",
    "        # 将预测值和真实值添加到列表中\n",
    "        y_true.extend(y.tolist())\n",
    "        y_pred.extend(outputs.squeeze().tolist())\n",
    "\n",
    "# 转换为NumPy数组\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "判断均方根误差（Root Mean Square Error，RMSE）是否可接受的标准通常是与具体的应用场景和数据特征相关的。没有一个通用的阈值来衡量是否可接受，因为它取决于问题的上下文和预期的误差范围。一种常见的方法是将RMSE与目标变量的范围进行比较。如果RMSE远远小于目标变量的范围，那么可以认为结果是可接受的。另一种方法是与先前的模型或基准模型进行比较，如果新模型的RMSE显著优于先前的模型或基准模型，那么结果可以认为是可接受的。此外，还应该考虑问题的实际需求和对误差的容忍度。有些应用可能对误差非常敏感，需要较低的RMSE值，而其他应用可能对误差更容忍，可以接受较高的RMSE值。\n",
    "\n",
    "判断平均绝对误差（Mean Absolute Error，MAE）是否可接受的标准也通常与具体的应用场景和数据特征相关。与均方根误差（RMSE）类似，没有一个通用的阈值来衡量是否可接受，因为它取决于问题的上下文和预期的误差范围。一种常见的方法是将MAE与目标变量的范围进行比较。如果MAE远远小于目标变量的范围，那么可以认为结果是可接受的。另一种方法是与先前的模型或基准模型进行比较，如果新模型的MAE显著优于先前的模型或基准模型，那么结果可以认为是可接受的。与RMSE类似，还应该考虑问题的实际需求和对误差的容忍度。有些应用可能对误差非常敏感，需要较低的MAE值，而其他应用可能对误差更容忍，可以接受较高的MAE值。\n",
    "\n",
    "决定系数（Coefficient of Determination），也称为R-squared（R²），用于评估回归模型的拟合优度。它表示模型能够解释目标变量方差的比例，取值范围从0到1，越接近1表示模型拟合得越好，越接近0表示模型拟合较差。对于决定系数，通常没有一个固定的阈值来判断结果是否可接受，因为它也取决于具体的应用场景和数据特征。一般来说，较高的决定系数意味着模型能够较好地解释目标变量的变异性，而较低的决定系数则表示模型的解释能力较弱。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方根误差: 0.39495930109779853\n",
      "平均绝对误差: 0.39418345001008775\n",
      "决定系数: 0.0\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f'均方根误差: {rmse}')\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f'平均绝对误差: {mae}')\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f'决定系数: {r2}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}